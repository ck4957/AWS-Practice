{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM2cVDGmGgv7s+0hM4sLET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ck4957/AWS-Practice/blob/main/reddit_scrap_wsb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk praw matplotlib pandas numpy"
      ],
      "metadata": {
        "id": "tLAN8_NAKHz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "import praw\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stocks = ['AAPL','ABBV','ABEV','ABNB','ABT','ACGL','ACI','ACM','ACN','ADBE','ADI','ADM','ADP','ADSK','AEE','AEG','AEM','AEP','AER','AESC','AFL','AGNCL','AGNCM','AGNCN','AGNCO','AGNCP','AGR','AIG','AJG','AKAM','ALB','ALC','ALGN','ALL','ALNY','AMAT','AMCR','AMD','AME','AMGN','AMH','AMP','AMT','AMX','AMZN','ANET','ANSS','AON','AOS','APA','APD','APH','APO','APP','APTV','AQNA','AQNB','AQNU','ARCC','ARE','ARES','ARGX','ARM','ASML','ASX','ATO','ATVI','AVB','AVGO','AVTR','AVY','AWK','AXON','AXP','AZN','AZO','AZPN','BA','BABA','BAC','BAH','BALL','BAM','BAP','BAX','BBD','BBDO','BBVA','BBY','BCE','BCH','BCS','BDX','BEKE','BEN','BG','BGNE','BHP','BIDU','BIIB','BILL','BIO','BIP','BK','BKDT','BKNG','BKR','BLDR','BLK','BMO','BMRN','BMY','BN','BNH','BNJ','BNS','BNTX','BP','BPYPM','BPYPN','BPYPO','BPYPP','BR','BRK/A','BRK/B','BRO','BSBR','BSX','BSY','BTI','BUD','BX','C','CAG','CAH','CARR','CASY','CAT','CB','CBOE','CBRE','CCEP','CCI','CCJ','CCK','CCL','CDAY','CDNS','CDW','CE','CEG','CELH','CF','CFG','CG','CHD','CHK','CHKEW','CHKEZ','CHKP','CHRW','CHT','CHTR','CI','CINF','CL','CLX','CM','CMCSA','CME','CMG','CMI','CMS','CNA','CNC','CNHI','CNI','CNP','CNQ','COF','COIN','COO','COP','COR','COST','CP','CPB','CPNG','CPRT','CPT','CQP','CRBG','CRH','CRL','CRM','CRWD','CSCO','CSGP','CSL','CSX','CTAS','CTRA','CTSH','CTVA','CVE','CVS','CVX','D','DAL','DASH','DB','DD','DDOG','DE','DECK','DELL','DEO','DFS','DG','DGX','DHI','DHR','DINO','DIS','DKNG','DLR','DLTR','DOV','DOW','DOX','DPZ','DRI','DT','DTE','DUK','DUKB','DVN','DXCM','E','EA','EBAY','EBR','EC','ECL','ED','EDR','EDU','EFX','EG','EIX','EL','ELP','ELS','ELV','EMR','ENB','ENPH','ENTG','EOG','EPAM','EPD','EQIX','EQNR','EQR','EQT','ERIC','ERIE','ES','ESS','ET','ETN','ETR','EVRG','EW','EXAS','EXC','EXPD','EXPE','EXR','F','FANG','FAST','FCNCA','FCX','FDS','FDX','FE','FERG','FI','FICO','FIS','FITB','FITBI','FITBO','FITBP','FLEX','FLT','FMS','FMX','FNF','FNV','FOX','FOXA','FSLR','FTNT','FTS','FTV','FWONA','FWONK','GD','GDDY','GE','GEHC','GEN','GFI','GFL','GFS','GGG','GIB','GILD','GIS','GL','GLPI','GLW','GM','GOLD','GOOG','GOOGL','GPC','GPN','GRAB','GRMN','GS','GSK','GWW','H','HAL','HBAN','HBANM','HBANP','HCA','HD','HDB','HEI','HES','HIG','HLN','HLT','HMC','HOLX','HON','HPE','HPQ','HRL','HSBC','HST','HSY','HTHT','HUBB','HUBS','HUM','HWM','IBM','IBN','ICE','ICLR','IDXX','IEX','IFF','IHG','ILMN','IMO','INCY','INFY','ING','INTC','INTU','INVH','IOT','IP','IPG','IQV','IR','IRM','ISRG','IT','ITUB','ITW','IX','J','JBHT','JBL','JCI','JD','JHX','JKHY','JNJ','JPM','K','KB','KDP','KEYS','KHC','KIM','KKR','KLAC','KMB','KMI','KMX','KNSL','KO','KOF','KR','KVUE','L','LBRDA','LBRDK','LCID','LDOS','LECO','LEGN','LEN','LH','LHX','LI','LII','LIN','LKQ','LLY','LMT','LNG','LNT','LOGI','LOW','LPLA','LRCX','LSCC','LULU','LUV','LVS','LW','LYB','LYG','LYV','MA','MAA','MANH','MAR','MAS','MCD','MCHP','MCK','MCO','MDB','MDLZ','MDT','MELI','MET','META','MFC','MFG','MGA','MGM','MKC','MKL','MLM','MMC','MMM','MNST','MO','MOH','MORN','MOS','MPC','MPLX','MPWR','MRK','MRNA','MRO','MRVL','MS','MSCI','MSFT','MSI','MT','MTB','MTCH','MTD','MU','MUFG','NBIX','NDAQ','NDSN','NEE','NEM','NET','NFLX','NGG','NI','NICE','NIMC','NIO','NKE','NMR','NOC','NOK','NOW','NSC','NTAP','NTES','NTR','NTRS','NU','NUE','NVDA','NVO','NVR','NVS','NWG','NWS','NWSA','NXPI','O','OC','ODFL','OKE','OKTA','OMC','ON','ORAN','ORCL','ORLY','OTIS','OVV','OWL','OXY','PAA','PAG','PANW','PARAP','PAYC','PAYX','PBA','PBR','PCAR','PCG','PCTY','PDD','PEG','PEP','PFE','PFG','PFH','PG','PGR','PH','PHG','PHM','PINS','PKG','PKX','PLD','PLTR','PM','PNC','PNR','PODD','POOL','PPG','PPL','PRU','PSA','PSTG','PSX','PTC','PUK','PWR','PXD','PYPL','QCOM','QRTEP','QSR','RACE','RBA','RBLX','RCI','RCL','RDY','REG','REGN','RELX','REXR','RF','RIO','RIVN','RJF','RKT','RMD','RNR','ROK','ROKU','ROL','ROP','ROST','RPM','RPRX','RS','RSG','RTO','RTX','RVTY','RY','RYAAY','RYAN','SAIA','SAN','SAP','SATX','SBAC','SBUX','SCCO','SCHW','SE','SGEN','SHEL','SHG','SHOP','SHW','SIRI','SJM','SLB','SLF','SLMBP','SMCI','SMFG','SNA','SNAP','SNN','SNOW','SNPS','SNY','SO','SOJE','SONY','SPG','SPGI','SPLK','SPOT','SQ','SQM','SRE','SREA','SRPT','SSNC','STE','STLA','STLD','STM','STT','STX','STZ','SU','SUI','SWK','SWKS','SYF','SYK','SYM','SYT','SYY','T','TAK','TAP','TBB','TBC','TCOM','TD','TDG','TDY','TEAM','TECH','TECK','TEF','TEL','TER','TEVA','TFC','TFII','TGT','TJX','TLK','TM','TME','TMO','TMUS','TPL','TRGP','TRI','TRMB','TROW','TRP','TRU','TRV','TS','TSCO','TSLA','TSM','TSN','TT','TTD','TTE','TTWO','TU','TVE','TW','TWLO','TXN','TXT','TYL','U','UAL','UBER','UBS','UDR','UGIC','UHAL','UL','ULTA','UMC','UNH','UNP','UPS','URI','USB','UTHR','V','VALE','VEEV','VFS','VICI','VIV','VLO','VLTO','VLYPO','VMC','VMW','VOD','VRSK','VRSN','VRT','VRTX','VST','VTR','VTRS','VZ','WAB','WAT','WBA','WBD','WCN','WDAY','WDC','WDS','WEC','WELL','WES','WFC','WIT','WLK','WM','WMB','WMG','WMT','WPC','WPM','WPP','WRB','WSM','WSO','WST','WTW','WY','WYNN','XEL','XOM','XP','XYL','YUM','YUMC','Z','ZBH','ZBRA','ZG','ZM','ZS','ZTO','ZTS']\n",
        "\n",
        "mag7 = ['AAPL', 'MSFT', 'META', 'GOOGL', 'NVDA', 'TSLA']\n",
        "\n",
        "reddit_read_only = praw.Reddit(client_id=\"\", client_secret=\"\",\t # your client secret\n",
        "\t\t\t\t\t\t\tuser_agent=\"my user agent\", check_for_async=False)\t # your user agent\n",
        "\n",
        "\n",
        "\n",
        "def readTopPosts(group):\n",
        "  for post in group.hot(limit=5):\n",
        "      print(post.title)\n",
        "      print()\n",
        "  # Display the name of the Subreddit\n",
        "\n",
        "\n",
        "  posts = group.top(time_filter = \"month\")\n",
        "  # Scraping the top posts of the current month\n",
        "\n",
        "  posts_dict = {\"Title\": [], \"Post Text\": [],\n",
        "                \"ID\": [], \"Score\": [],\n",
        "                \"Total Comments\": [], \"Post URL\": []\n",
        "                }\n",
        "  for post in posts:\n",
        "      # Title of each post\n",
        "      posts_dict[\"Title\"].append(post.title)\n",
        "\n",
        "      # Text inside a post\n",
        "      posts_dict[\"Post Text\"].append(post.selftext)\n",
        "\n",
        "      # Unique ID of each post\n",
        "      posts_dict[\"ID\"].append(post.id)\n",
        "\n",
        "      # The score of a post\n",
        "      posts_dict[\"Score\"].append(post.score)\n",
        "\n",
        "      # Total number of comments inside the post\n",
        "      posts_dict[\"Total Comments\"].append(post.num_comments)\n",
        "\n",
        "      # URL of each post\n",
        "      posts_dict[\"Post URL\"].append(post.url)\n",
        "\n",
        "  # Saving the data in a pandas dataframe\n",
        "  top_posts = pd.DataFrame(posts_dict)\n",
        "  print(top_posts)\n",
        "\n",
        "\n",
        "def commentSentiment(ticker, reddit, urlT):\n",
        "    #print(ticker, reddit, urlT)\n",
        "    subComments = []\n",
        "    bodyComment = []\n",
        "    try:\n",
        "        check = reddit.submission(url=urlT)\n",
        "        #print(check)\n",
        "        subComments = check.comments\n",
        "        #print(\"Total SubComment:\",len(subComments))\n",
        "\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "    for comment in subComments:\n",
        "        try:\n",
        "            bodyComment.append(comment.body)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    sia = SIA()\n",
        "    results = []\n",
        "    for line in bodyComment:\n",
        "        scores = sia.polarity_scores(line)\n",
        "        scores['headline'] = line\n",
        "\n",
        "        results.append(scores)\n",
        "\n",
        "    #print(\"Total headLineResults:\",len(results))\n",
        "\n",
        "    df =pd.DataFrame.from_records(results)\n",
        "    df.head()\n",
        "    df['label'] = 0\n",
        "\n",
        "    try:\n",
        "        df.loc[df['compound'] > 0.1, 'label'] = 1\n",
        "        df.loc[df['compound'] < -0.1, 'label'] = -1\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "    averageScore = 0\n",
        "    position = 0\n",
        "    while position < len(df.label)-1:\n",
        "        averageScore = averageScore + df.label[position]\n",
        "        position += 1\n",
        "    averageScore = averageScore/len(df.label)\n",
        "\n",
        "    return(averageScore)\n",
        "\n",
        "def latestComment(ticker, reddit, urlT):\n",
        "    subComments = []\n",
        "    updateDates = []\n",
        "    try:\n",
        "        check = reddit.submission(url=urlT)\n",
        "        subComments = check.comments\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "    for comment in subComments:\n",
        "        try:\n",
        "            updateDates.append(comment.created_utc)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    updateDates.sort()\n",
        "    return(updateDates[-1])\n",
        "\n",
        "def get_date(date):\n",
        "    return dt.datetime.fromtimestamp(date)\n",
        "\n"
      ],
      "metadata": {
        "id": "DFCoNNR6MGjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSubmissions(group, tickerList, limit_search):\n",
        "\n",
        "  submission_statistics = []\n",
        "  d = {}\n",
        "  for ticker in tickerList:\n",
        "      for submission in group.search(ticker, limit=limit_search):\n",
        "          #print(submission, submission.domain)\n",
        "          if submission.domain != \"self.wallstreetbets\":\n",
        "              continue\n",
        "          d = {}\n",
        "          d['ticker'] = ticker\n",
        "          d['num_comments'] = submission.num_comments\n",
        "          d['comment_sentiment_average'] = commentSentiment(ticker, reddit_read_only, submission.url)\n",
        "          #print(\"Sent Avg\", d['comment_sentiment_average'])\n",
        "          if d['comment_sentiment_average'] == 0.000000:\n",
        "              continue\n",
        "          d['latest_comment_date'] = latestComment(ticker, reddit_read_only, submission.url)\n",
        "          d['score'] = submission.score\n",
        "          d['upvote_ratio'] = submission.upvote_ratio\n",
        "          #print(d)\n",
        "          d['created_date'] = submission.created_utc\n",
        "          d['domain'] = submission.domain\n",
        "          d['num_crossposts'] = submission.num_crossposts\n",
        "          d['author'] = submission.author\n",
        "          submission_statistics.append(d)\n",
        "  return submission_statistics"
      ],
      "metadata": {
        "id": "Xw9ALkOMMJdr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yzkAg8eKB-r"
      },
      "outputs": [],
      "source": [
        "wsb_subreddit = reddit_read_only.subreddit(\"wallstreetbets\")\n",
        "\n",
        "wsb_data = getSubmissions(wsb_subreddit, mag7, 10)\n",
        "print(\"Total Submissions:\", len(wsb_data))\n",
        "\n",
        "if len(wsb_data) > 0:\n",
        "  dfSentimentStocks = pd.DataFrame(wsb_data)\n",
        "\n",
        "  _timestampcreated = dfSentimentStocks[\"created_date\"].apply(get_date)\n",
        "  dfSentimentStocks = dfSentimentStocks.assign(timestamp = _timestampcreated)\n",
        "\n",
        "  _timestampcomment = dfSentimentStocks[\"latest_comment_date\"].apply(get_date)\n",
        "  dfSentimentStocks = dfSentimentStocks.assign(commentdate = _timestampcomment)\n",
        "\n",
        "  dfSentimentStocks.sort_values(\"latest_comment_date\", axis = 0, ascending = True,inplace = True, na_position ='last')\n",
        "  dfSentimentStocks.author.value_counts()\n",
        "  dfSentimentStocks.to_csv('Reddit_Sentiment_Equity.csv', index=False)\n",
        "  #print(dfSentimentStocks)\n"
      ]
    }
  ]
}
